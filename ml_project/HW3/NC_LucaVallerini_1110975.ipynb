{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HOMEWORK #3\n",
    "\n",
    "### Non linear models for classification \n",
    "\n",
    "In this notebook we are going to explore the use of SVM and Neural Networks (NNs) for image classification. We are going to use the famous MNIST dataset, that is a dataset of handwritten digits. We get the data from mldata.org, that is a public repository for machine learning data.\n",
    "\n",
    "The dataset consists of 70,000 images of handwritten digits (i.e., 0, 1, ... 9). Each image is 28 pixels by 28 pixels and we can think of it as a vector of 28x28 = 784 numbers. Each number is an integer between 0 and 255. For each image we have the corresponding label (i.e., 0, 1, ..., 9)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#load the required packages\n",
    "\n",
    "%matplotlib inline  \n",
    "\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import sklearn\n",
    "from sklearn.datasets import fetch_mldata\n",
    "from sklearn.neural_network import MLPClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TO DO 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#fix your ID (\"numero di matricola\") and the seed for random generator\n",
    "ID = 1110975\n",
    "np.random.seed(ID)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's load the dataset. 'data' contains the input, 'target' contains the label. We normalize the data by dividing each value by 255 so that each value is in [0,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#load the MNIST dataset and let's normalize the features so that each value is in [0,1]\n",
    "mnist = fetch_mldata(\"MNIST original\")\n",
    "# rescale the data\n",
    "X, y = mnist.data / 255., mnist.target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we split into training and test. Make sure that each label is present at least 10 times\n",
    "in training. If it is not, then keep adding permutations to the initial data until this \n",
    "happens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Labels and frequencies in training dataset: \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[  0.,  41.],\n",
       "       [  1.,  67.],\n",
       "       [  2.,  51.],\n",
       "       [  3.,  52.],\n",
       "       [  4.,  47.],\n",
       "       [  5.,  41.],\n",
       "       [  6.,  50.],\n",
       "       [  7.,  58.],\n",
       "       [  8.,  50.],\n",
       "       [  9.,  43.]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#random permute the data and split into training and test taking the first 500\n",
    "#data samples as training and the rests as test\n",
    "permutation = np.random.permutation(X.shape[0])\n",
    "\n",
    "X = X[permutation]\n",
    "y = y[permutation]\n",
    "\n",
    "m_training = 500\n",
    "\n",
    "X_train, X_test = X[:m_training], X[m_training:]\n",
    "y_train, y_test = y[:m_training], y[m_training:]\n",
    "\n",
    "print(\"Labels and frequencies in training dataset: \")\n",
    "sp.stats.itemfreq(y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now provide a function to print an image in a dataset, the corresponding true label, and the index of the image in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#function for plotting a digit and printing the corresponding label\n",
    "def plot_digit(X_matrix, labels, index):\n",
    "    print(\"INPUT:\")\n",
    "    plt.imshow(\n",
    "        X_matrix[index].reshape(28,28),\n",
    "        cmap          = plt.cm.gray_r,\n",
    "        interpolation = \"nearest\"\n",
    "    )\n",
    "    plt.show()\n",
    "    print(\"LABEL: %i\"%labels[index])\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As an example, let's print the 100-th image in X_train and the 40,000-th image in X_test and their true labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INPUT:\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nRSjrJCvF/gAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f306c943b90>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LABEL: 0\n",
      "INPUT:\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nCD8Q1P8Dju4Fjzb1PTMAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f3094107bd0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LABEL: 8\n"
     ]
    }
   ],
   "source": [
    "#let's try the plotting function\n",
    "plot_digit(X_train,y_train,100)\n",
    "plot_digit(X_test,y_test,40000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TO DO 1\n",
    "Run SVM with cross validation to pick a kernel and values of parameters. Use a 5-fold cross-validation to pick the best kernel and choice of parameters. We provide some potential choice for parameters, but change the grid if needed (e.g., it takes too long). For the SVM for classificarion use SVC from sklearn.svm; for the grid search we suggest you use GridSearchCV from sklearn.model_selection, but you can implement your own cross-validation for model selection if you prefer.\n",
    "\n",
    "Print the best parameters used as well as the best score obtained by the ``optimal'' model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RESULTS FOR LINEAR KERNEL\n",
      "\n",
      "Best parameters set found:\n",
      "{'C': 1}\n",
      "Score with best parameters:\n",
      "0.818\n",
      "\n",
      "All scores on the grid:\n",
      "[ 0.818  0.818  0.818]\n",
      "\n",
      "RESULTS FOR POLY DEGREE=2 KERNEL\n",
      "\n",
      "Best parameters set found:\n",
      "{'C': 1, 'gamma': 0.1}\n",
      "Score with best parameters:\n",
      "0.824\n",
      "\n",
      "All scores on the grid:\n",
      "[ 0.792  0.824  0.824  0.824  0.824  0.824  0.824  0.824  0.824]\n",
      "\n",
      "RESULTS FOR rbf KERNEL\n",
      "\n",
      "Best parameters set found:\n",
      "{'C': 10, 'gamma': 0.01}\n",
      "Score with best parameters:\n",
      "0.86\n",
      "\n",
      "All scores on the grid:\n",
      "[ 0.848  0.59   0.134  0.86   0.612  0.134  0.86   0.612  0.134]\n"
     ]
    }
   ],
   "source": [
    "#import SVC\n",
    "from sklearn.svm import SVC\n",
    "#import for Cross-Validation\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# best parameters for SVM with different kernel\n",
    "# index 0: linear, index 1: polynomial, index 2: RBF\n",
    "best_params_SVM = [] \n",
    "\n",
    "# parameters for linear SVM\n",
    "parameters = {'C': [1, 10, 100]}\n",
    "\n",
    "#run linear SVM\n",
    "linear_SVM = SVC(kernel='linear')\n",
    "\n",
    "### FIT MODEL USING 5-fold CV\n",
    "cv_linear = GridSearchCV(linear_SVM, parameters, cv=5)\n",
    "cv_linear.fit(X_train, y_train)\n",
    "best_params_SVM.append([cv_linear.best_score_, cv_linear.best_params_, 'linear'])\n",
    "\n",
    "print ('RESULTS FOR LINEAR KERNEL\\n')\n",
    "print(\"Best parameters set found:\")\n",
    "print cv_linear.best_params_\n",
    "print(\"Score with best parameters:\")\n",
    "print cv_linear.best_score_ \n",
    "print(\"\\nAll scores on the grid:\")\n",
    "print cv_linear.cv_results_['mean_test_score']\n",
    "\n",
    "# parameters for poly with degree 2 kernel\n",
    "parameters = {'C': [1, 10, 100],'gamma':[0.01,0.1,1.]}\n",
    "\n",
    "#run SVM with poly of degree 2 kernel\n",
    "poly2_SVM = SVC(kernel='poly',degree=2)\n",
    "\n",
    "### DO THE SAME AS ABOVE FOR POLYNOMIAL KERNEL WITH DEGREE=2\n",
    "cv_poly = GridSearchCV(poly2_SVM, parameters, cv=5)\n",
    "cv_poly.fit(X_train, y_train)\n",
    "best_params_SVM.append([cv_poly.best_score_, cv_poly.best_params_, 'poly'])\n",
    "\n",
    "print ('\\nRESULTS FOR POLY DEGREE=2 KERNEL\\n')\n",
    "print(\"Best parameters set found:\")\n",
    "print cv_poly.best_params_\n",
    "print(\"Score with best parameters:\")\n",
    "print cv_poly.best_score_\n",
    "print(\"\\nAll scores on the grid:\")\n",
    "print cv_poly.cv_results_['mean_test_score']\n",
    "\n",
    "# parameters for rbf SVM\n",
    "parameters = {'C': [1, 10, 100],'gamma':[0.01,0.1,1.]}\n",
    "\n",
    "#run SVM with rbf kernel\n",
    "rbf_SVM = SVC(kernel='rbf')\n",
    "### ADD CODE: FIT MODEL USING 5-fold CV\n",
    "cv_rbf = GridSearchCV(rbf_SVM, parameters, cv=5)\n",
    "cv_rbf.fit(X_train, y_train)\n",
    "best_params_SVM.append([cv_rbf.best_score_, cv_rbf.best_params_, 'rbf'])\n",
    "\n",
    "print ('\\nRESULTS FOR rbf KERNEL\\n')\n",
    "print(\"Best parameters set found:\")\n",
    "print cv_rbf.best_params_\n",
    "print(\"Score with best parameters:\")\n",
    "print cv_rbf.best_score_\n",
    "print(\"\\nAll scores on the grid:\")\n",
    "print cv_rbf.cv_results_['mean_test_score']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TO DO 2\n",
    "For the \"best\" SVM kernel and choice of parameters from above, train the model on the entire training set and measure the training error. Also make predictions on the test set and measure the test error. Print the training and the test error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'kernel': 'rbf', 'C': 10, 'gamma': 0.01}\n",
      "Best SVM training error: 0.000000\n",
      "Best SVM test error: 0.109928\n"
     ]
    }
   ],
   "source": [
    "# retrieve the parameters for the best SVM model from CV\n",
    "best_score, best_index = 0., 0\n",
    "for i in range(len(best_params_SVM)):\n",
    "    if best_params_SVM[i][0] > best_score:\n",
    "        best_score = best_params_SVM[i][0]\n",
    "        best_index = i\n",
    "        \n",
    "best_parameters = {}\n",
    "best_parameters = best_params_SVM[best_index][1]\n",
    "best_parameters['kernel'] = best_params_SVM[best_index][2] \n",
    "\n",
    "# get training and test error for the best SVM model from CV\n",
    "best_SVM = SVC(best_parameters)\n",
    "best_SVM.fit(X_train, y_train)\n",
    "\n",
    "training_error = 1. - best_SVM.score(X_train,y_train)\n",
    "test_error = 1. - best_SVM.score(X_test,y_test)\n",
    "\n",
    "print (\"Best SVM training error: %f\" % training_error)\n",
    "print (\"Best SVM test error: %f\" % test_error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TO DO 3\n",
    "Now we use feed-forward neural networks for classification. You can use the Multi-Layer-Perceptron (the feedforward/multilayer structure we have seen in class, see http://scikit-learn.org/stable/modules/neural_networks_supervised.html).\n",
    "\n",
    "Note that we fix the starting random state so to make the runs reproducible. Use max_iter=20, alpha=1e-4, solver='sgd', tol=1e-4, learning_rate_init=.1. Pick few architectures and use the default activation function (ReLU).\n",
    "\n",
    "REMARK: For solver ‘sgd’ max_iter determines the number of epochs (how many times each data point will be used), not the number of gradient steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/luca/anaconda2/lib/python2.7/site-packages/sklearn/neural_network/multilayer_perceptron.py:564: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (20) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "RESULTS FOR NN\n",
      "\n",
      "Best parameters set found:\n",
      "{'hidden_layer_sizes': (50,)}\n",
      "Score with best parameters:\n",
      "0.67\n",
      "\n",
      "All scores on the grid:\n",
      "[ 0.504  0.67   0.424  0.564]\n"
     ]
    }
   ],
   "source": [
    "# test different architectures: \n",
    "# - 1 hidden layer with 10 nodes, \n",
    "# - 1 hidden layer with 50 nodes, \n",
    "# - 2 hidden layer with 10 nodes each, \n",
    "# - 2 hidden layer with 50 nodes each\n",
    "# feel free to change this and test more/different  structures\n",
    "\n",
    "parameters = {'hidden_layer_sizes': [(10,), (50,), (10,10,), (50,50,)]}\n",
    "\n",
    "mlp = MLPClassifier(max_iter=20, alpha=1e-4,\n",
    "                    solver='sgd', tol=1e-4, random_state=ID,\n",
    "                    learning_rate_init=.1)\n",
    "\n",
    "# ADD CODE: TRAIN NN & FIND BEST STRUCTURE (HYDDEN LAYER SIZE) USING 5-FOLD CV\n",
    "cv_nn = GridSearchCV(mlp, parameters, cv=5)\n",
    "cv_nn.fit(X_train, y_train)\n",
    "\n",
    "print ('\\nRESULTS FOR NN\\n')\n",
    "print(\"Best parameters set found:\")\n",
    "print cv_nn.best_params_\n",
    "print(\"Score with best parameters:\")\n",
    "print cv_nn.best_score_\n",
    "print(\"\\nAll scores on the grid:\")\n",
    "print cv_nn.cv_results_['mean_test_score']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TO DO 4\n",
    "\n",
    "Now get training and test error for a NN with best parameters from above. Use verbose=True\n",
    "in input so to see how loss changes in iterations (see how this changs if the number of iterations is changed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 2.23201842\n",
      "Iteration 2, loss = 1.71665673\n",
      "Iteration 3, loss = 1.11738868\n",
      "Iteration 4, loss = 0.72773522\n",
      "Iteration 5, loss = 0.52198125\n",
      "Iteration 6, loss = 0.42569321\n",
      "Iteration 7, loss = 0.34290507\n",
      "Iteration 8, loss = 0.30266387\n",
      "Iteration 9, loss = 0.24538060\n",
      "Iteration 10, loss = 0.20705682\n",
      "Iteration 11, loss = 0.16862676\n",
      "Iteration 12, loss = 0.14460576\n",
      "Iteration 13, loss = 0.12528509\n",
      "Iteration 14, loss = 0.09712119\n",
      "Iteration 15, loss = 0.08414897\n",
      "Iteration 16, loss = 0.07293109\n",
      "Iteration 17, loss = 0.06166876\n",
      "Iteration 18, loss = 0.05531551\n",
      "Iteration 19, loss = 0.04767693\n",
      "Iteration 20, loss = 0.04313744\n",
      "\n",
      "RESULTS FOR BEST NN\n",
      "\n",
      "Best NN training error: 0.002000\n",
      "Best NN test error: 0.159237\n"
     ]
    }
   ],
   "source": [
    "# get training and test error for the best NN model from CV\n",
    "parameters_nn = {'max_iter': 20, 'alpha': 1e-4, 'hidden_layer_sizes': cv_nn.best_params_['hidden_layer_sizes'],\n",
    "                    'solver': 'sgd', 'tol': 1e-4, 'random_state': ID, 'learning_rate_init': .1, 'verbose': True}\n",
    "mlp = MLPClassifier(**parameters_nn) \n",
    "\n",
    "# ADD CODE: FIT MODEL & COMPUTE TRAINING AND TEST ERRORS\n",
    "mlp.fit(X_train, y_train)\n",
    "\n",
    "training_error = 1. - mlp.score(X_train, y_train)\n",
    "test_error = 1. - mlp.score(X_test, y_test)\n",
    "\n",
    "print ('\\nRESULTS FOR BEST NN\\n')\n",
    "\n",
    "print (\"Best NN training error: %f\" % training_error)\n",
    "print (\"Best NN test error: %f\" % test_error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TO DO 5 \n",
    "According to the cross-validation results, would you choose SVMs or NNs when 500 data points are available for training? Is this a good choice, given the results on the test set?\n",
    "\n",
    "### ANSWER \n",
    "Based on the results on cross-validation I would choose SVM since it scores better (with all kernels too). Also, given the results from the test set, SVM achieves a lower error than NN.\n",
    "\n",
    "Note: these results may change based on the actual data in the training and test set. On different trials NN performed better with lower test error than SVM. Nonetheless, SVM had always achieved a better score."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TO DO  6\n",
    "For the sake of illustration, find and plot  (using function $plot\\_digit$ from above)  a digit that is missclassified by NN and correctly classified by SVM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INPUT:\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nkiL8QFKEH0iK8ANJEX4gKcIPJPX/Q1TKjxu9D8kAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f306221e490>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LABEL: 5\n",
      "('SVM prediction: ', 5.0)\n",
      "('NN prediction: ', 9.0)\n"
     ]
    }
   ],
   "source": [
    "SVM_prediction = best_SVM.predict(X_test)\n",
    "NN_prediction = mlp.predict(X_test)\n",
    "i = 0\n",
    "found = False\n",
    "while ((not found) and (i<len(y_test))):\n",
    "    if (SVM_prediction[i] == y_test[i]) and (NN_prediction[i] != y_test[i]):\n",
    "        plot_digit(X_test, y_test, i)\n",
    "        print(\"SVM prediction: \", SVM_prediction[i])\n",
    "        print(\"NN prediction: \",NN_prediction[i])\n",
    "        found = True\n",
    "    else:\n",
    "        i += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TO DO 7 (more data)\n",
    "\n",
    "Now let's do the same but using 60000 data points for training. For SVM we are only going to use the best model we got using 500 data points. For NNs we use the same architectures as before, but you can try more if you want!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Labels and frequencies in training dataset: \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[  0.00000000e+00,   5.91400000e+03],\n",
       "       [  1.00000000e+00,   6.77500000e+03],\n",
       "       [  2.00000000e+00,   5.99800000e+03],\n",
       "       [  3.00000000e+00,   6.14500000e+03],\n",
       "       [  4.00000000e+00,   5.88300000e+03],\n",
       "       [  5.00000000e+00,   5.41800000e+03],\n",
       "       [  6.00000000e+00,   5.85900000e+03],\n",
       "       [  7.00000000e+00,   6.23800000e+03],\n",
       "       [  8.00000000e+00,   5.81600000e+03],\n",
       "       [  9.00000000e+00,   5.95400000e+03]])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = X[permutation]\n",
    "y = y[permutation]\n",
    "\n",
    "m_training = 60000\n",
    "\n",
    "X_train, X_test = X[:m_training], X[m_training:]\n",
    "y_train, y_test = y[:m_training], y[m_training:]\n",
    "\n",
    "print(\"Labels and frequencies in training dataset: \")\n",
    "sp.stats.itemfreq(y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try to use SVM with parameters obtained from the best model for m_training = 500 (using the new, larger training dataset to fit the model) Since it may take a long time to run, you can decide to just let it run for some time and stop it if it does not complete. If you decide to do this, report it in the last TO DO cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best SVM training error: 0.098033\n",
      "Best SVM test error: 0.104600\n"
     ]
    }
   ],
   "source": [
    "# get training and test error for the best SVM model from CV\n",
    "parameters_SVM_large = best_parameters\n",
    "parameters_SVM_large['max_iter'] = 50 # since it takes too long, I set an upper bound to the number of iterations\n",
    "best_SVM_large = SVC(**parameters_SVM_large)\n",
    "best_SVM_large.fit(X_train, y_train)\n",
    "\n",
    "training_error = 1. - best_SVM_large.score(X_train, y_train)\n",
    "test_error = 1. - best_SVM_large.score(X_test, y_test)\n",
    "\n",
    "svm_or_lr = {'svm': test_error} # I'll use it later to choose between SVM or LR\n",
    "\n",
    "print (\"Best SVM training error: %f\" % training_error)\n",
    "print (\"Best SVM test error: %f\" % test_error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just for comparison, since it may not be possible to learn a SVM, let's also use logistic regression (with standard parameters from scikit-learn, i.e. some regularization is included)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best logistic regression training error: 0.071900\n",
      "Best logistic regression test error: 0.081300\n"
     ]
    }
   ],
   "source": [
    "from sklearn import linear_model\n",
    "\n",
    "logistic_regression_large = linear_model.LogisticRegression()\n",
    "logistic_regression_large.fit(X_train, y_train)\n",
    "\n",
    "training_error = 1. - logistic_regression_large.score(X_train, y_train)\n",
    "test_error = 1. - logistic_regression_large.score(X_test, y_test)\n",
    "\n",
    "svm_or_lr['lr'] = test_error # I'll use it later to choose between SVM or LR\n",
    "\n",
    "print (\"Best logistic regression training error: %f\" % training_error)\n",
    "print (\"Best logistic regression test error: %f\" % test_error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now instead we learn NNs. Below we report the same architectures as before, feel free to try more different ones if you want, or less if it takes too much time. Describe you decisions below. (We suggest that you use 'verbose=True' so have an idea of how long it takes to run 1 iteration.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.52256385\n",
      "Iteration 2, loss = 0.34114339\n",
      "Iteration 3, loss = 0.31589998\n",
      "Iteration 4, loss = 0.28745057\n",
      "Iteration 5, loss = 0.27874590\n",
      "Iteration 6, loss = 0.27030788\n",
      "Iteration 7, loss = 0.26619926\n",
      "Iteration 8, loss = 0.26115764\n",
      "Iteration 9, loss = 0.25870638\n",
      "Iteration 10, loss = 0.25791131\n",
      "Iteration 11, loss = 0.25243006\n",
      "Iteration 12, loss = 0.25002025\n",
      "Iteration 13, loss = 0.24733372\n",
      "Iteration 14, loss = 0.24629462\n",
      "Iteration 15, loss = 0.24403880\n",
      "Iteration 16, loss = 0.24095343\n",
      "Iteration 17, loss = 0.24045521\n",
      "Iteration 18, loss = 0.24016071\n",
      "Iteration 19, loss = 0.23924963\n",
      "Iteration 20, loss = 0.23800554\n",
      "Iteration 1, loss = 0.52290915\n",
      "Iteration 2, loss = 0.34276639\n",
      "Iteration 3, loss = 0.32518576\n",
      "Iteration 4, loss = 0.31292321\n",
      "Iteration 5, loss = 0.30506484\n",
      "Iteration 6, loss = 0.30018876\n",
      "Iteration 7, loss = 0.29539296\n",
      "Iteration 8, loss = 0.29100992\n",
      "Iteration 9, loss = 0.28677965\n",
      "Iteration 10, loss = 0.28331260\n",
      "Iteration 11, loss = 0.28022081\n",
      "Iteration 12, loss = 0.27910549\n",
      "Iteration 13, loss = 0.27669331\n",
      "Iteration 14, loss = 0.27513305\n",
      "Iteration 15, loss = 0.27236472\n",
      "Iteration 16, loss = 0.27041063\n",
      "Iteration 17, loss = 0.26951028\n",
      "Iteration 18, loss = 0.26953526\n",
      "Iteration 19, loss = 0.26741356\n",
      "Iteration 20, loss = 0.26743701\n",
      "Iteration 1, loss = 0.52142762\n",
      "Iteration 2, loss = 0.33762996\n",
      "Iteration 3, loss = 0.30771394\n",
      "Iteration 4, loss = 0.28407982\n",
      "Iteration 5, loss = 0.27121200\n",
      "Iteration 6, loss = 0.26632908\n",
      "Iteration 7, loss = 0.26180876\n",
      "Iteration 8, loss = 0.25975402\n",
      "Iteration 9, loss = 0.25520963\n",
      "Iteration 10, loss = 0.25114132\n",
      "Iteration 11, loss = 0.24932437\n",
      "Iteration 12, loss = 0.24934984\n",
      "Iteration 13, loss = 0.24576362\n",
      "Iteration 14, loss = 0.24327692\n",
      "Iteration 15, loss = 0.24320045\n",
      "Iteration 16, loss = 0.24059708\n",
      "Iteration 17, loss = 0.24031681\n",
      "Iteration 18, loss = 0.23805085\n",
      "Iteration 19, loss = 0.23778270\n",
      "Iteration 20, loss = 0.23536063\n",
      "Iteration 1, loss = 0.51101734\n",
      "Iteration 2, loss = 0.31885129\n",
      "Iteration 3, loss = 0.29614800\n",
      "Iteration 4, loss = 0.27876722\n",
      "Iteration 5, loss = 0.27259571\n",
      "Iteration 6, loss = 0.26471284\n",
      "Iteration 7, loss = 0.25915746\n",
      "Iteration 8, loss = 0.25564999\n",
      "Iteration 9, loss = 0.25150564\n",
      "Iteration 10, loss = 0.24969174\n",
      "Iteration 11, loss = 0.25328978\n",
      "Iteration 12, loss = 0.24410416\n",
      "Iteration 13, loss = 0.23972854\n",
      "Iteration 14, loss = 0.23709308\n",
      "Iteration 15, loss = 0.25820735\n",
      "Iteration 16, loss = 0.28074922\n",
      "Iteration 17, loss = 0.24493638\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.54172980\n",
      "Iteration 2, loss = 0.34117631\n",
      "Iteration 3, loss = 0.33690460\n",
      "Iteration 4, loss = 0.28073888\n",
      "Iteration 5, loss = 0.29476674\n",
      "Iteration 6, loss = 0.26735572\n",
      "Iteration 7, loss = 0.26593564\n",
      "Iteration 8, loss = 0.26153990\n",
      "Iteration 9, loss = 0.27872334\n",
      "Iteration 10, loss = 0.25520457\n",
      "Iteration 11, loss = 0.25326121\n",
      "Iteration 12, loss = 0.25051691\n",
      "Iteration 13, loss = 0.25908226\n",
      "Iteration 14, loss = 0.24954137\n",
      "Iteration 15, loss = 0.24894196\n",
      "Iteration 16, loss = 0.24301675\n",
      "Iteration 17, loss = 0.24606035\n",
      "Iteration 18, loss = 0.25009318\n",
      "Iteration 19, loss = 0.24067877\n",
      "Iteration 20, loss = 0.23958807\n",
      "Iteration 1, loss = 0.35581102\n",
      "Iteration 2, loss = 0.16523123\n",
      "Iteration 3, loss = 0.12253064\n",
      "Iteration 4, loss = 0.09940834\n",
      "Iteration 5, loss = 0.08682097\n",
      "Iteration 6, loss = 0.07501155\n",
      "Iteration 7, loss = 0.06524059\n",
      "Iteration 8, loss = 0.05751745\n",
      "Iteration 9, loss = 0.05286748\n",
      "Iteration 10, loss = 0.04664619\n",
      "Iteration 11, loss = 0.04256936\n",
      "Iteration 12, loss = 0.03820740\n",
      "Iteration 13, loss = 0.03497715\n",
      "Iteration 14, loss = 0.03047577\n",
      "Iteration 15, loss = 0.02797894\n",
      "Iteration 16, loss = 0.02465186\n",
      "Iteration 17, loss = 0.02226879\n",
      "Iteration 18, loss = 0.02000168\n",
      "Iteration 19, loss = 0.01831646\n",
      "Iteration 20, loss = 0.01666060\n",
      "Iteration 1, loss = 0.35603828\n",
      "Iteration 2, loss = 0.16512750\n",
      "Iteration 3, loss = 0.12110118\n",
      "Iteration 4, loss = 0.09599859\n",
      "Iteration 5, loss = 0.08090411\n",
      "Iteration 6, loss = 0.06932984\n",
      "Iteration 7, loss = 0.06124137\n",
      "Iteration 8, loss = 0.05339322\n",
      "Iteration 9, loss = 0.04740650\n",
      "Iteration 10, loss = 0.04178129\n",
      "Iteration 11, loss = 0.03714699\n",
      "Iteration 12, loss = 0.03343562\n",
      "Iteration 13, loss = 0.03047423\n",
      "Iteration 14, loss = 0.02636469\n",
      "Iteration 15, loss = 0.02402124\n",
      "Iteration 16, loss = 0.02127784\n",
      "Iteration 17, loss = 0.01802771\n",
      "Iteration 18, loss = 0.01580796\n",
      "Iteration 19, loss = 0.01381199\n",
      "Iteration 20, loss = 0.01330004\n",
      "Iteration 1, loss = 0.35116071\n",
      "Iteration 2, loss = 0.16477890\n",
      "Iteration 3, loss = 0.12376307\n",
      "Iteration 4, loss = 0.09935379\n",
      "Iteration 5, loss = 0.08325183\n",
      "Iteration 6, loss = 0.07169091\n",
      "Iteration 7, loss = 0.06220497\n",
      "Iteration 8, loss = 0.05316908\n",
      "Iteration 9, loss = 0.04735636\n",
      "Iteration 10, loss = 0.04218513\n",
      "Iteration 11, loss = 0.03817600\n",
      "Iteration 12, loss = 0.03318536\n",
      "Iteration 13, loss = 0.02913446\n",
      "Iteration 14, loss = 0.02806567\n",
      "Iteration 15, loss = 0.02388778\n",
      "Iteration 16, loss = 0.02064534\n",
      "Iteration 17, loss = 0.01801984\n",
      "Iteration 18, loss = 0.01543519\n",
      "Iteration 19, loss = 0.01368696\n",
      "Iteration 20, loss = 0.01390461\n",
      "Iteration 1, loss = 0.35306169\n",
      "Iteration 2, loss = 0.16389323\n",
      "Iteration 3, loss = 0.12264431\n",
      "Iteration 4, loss = 0.09754611\n",
      "Iteration 5, loss = 0.08650543\n",
      "Iteration 6, loss = 0.07174364\n",
      "Iteration 7, loss = 0.06200411\n",
      "Iteration 8, loss = 0.05403166\n",
      "Iteration 9, loss = 0.04897730\n",
      "Iteration 10, loss = 0.04264902\n",
      "Iteration 11, loss = 0.03924899\n",
      "Iteration 12, loss = 0.03547872\n",
      "Iteration 13, loss = 0.03113034\n",
      "Iteration 14, loss = 0.02812446\n",
      "Iteration 15, loss = 0.02636367\n",
      "Iteration 16, loss = 0.02253130\n",
      "Iteration 17, loss = 0.01989809\n",
      "Iteration 18, loss = 0.01743638\n",
      "Iteration 19, loss = 0.01575490\n",
      "Iteration 20, loss = 0.01369369\n",
      "Iteration 1, loss = 0.36143747\n",
      "Iteration 2, loss = 0.17614454\n",
      "Iteration 3, loss = 0.12794128\n",
      "Iteration 4, loss = 0.10162080\n",
      "Iteration 5, loss = 0.08470979\n",
      "Iteration 6, loss = 0.07473355\n",
      "Iteration 7, loss = 0.06907791\n",
      "Iteration 8, loss = 0.05863646\n",
      "Iteration 9, loss = 0.05181090\n",
      "Iteration 10, loss = 0.04640361\n",
      "Iteration 11, loss = 0.04119469\n",
      "Iteration 12, loss = 0.03556377\n",
      "Iteration 13, loss = 0.03410550\n",
      "Iteration 14, loss = 0.02981860\n",
      "Iteration 15, loss = 0.02644627\n",
      "Iteration 16, loss = 0.02438244\n",
      "Iteration 17, loss = 0.02146432\n",
      "Iteration 18, loss = 0.01858842\n",
      "Iteration 19, loss = 0.01634811\n",
      "Iteration 20, loss = 0.01414279\n",
      "Iteration 1, loss = 0.56355924\n",
      "Iteration 2, loss = 0.30050753\n",
      "Iteration 3, loss = 0.27570445\n",
      "Iteration 4, loss = 0.26371950\n",
      "Iteration 5, loss = 0.25228555\n",
      "Iteration 6, loss = 0.24517090\n",
      "Iteration 7, loss = 0.23525089\n",
      "Iteration 8, loss = 0.23202310\n",
      "Iteration 9, loss = 0.22857568\n",
      "Iteration 10, loss = 0.22370208\n",
      "Iteration 11, loss = 0.21876354\n",
      "Iteration 12, loss = 0.21726314\n",
      "Iteration 13, loss = 0.21263005\n",
      "Iteration 14, loss = 0.20851581\n",
      "Iteration 15, loss = 0.20702183\n",
      "Iteration 16, loss = 0.20399273\n",
      "Iteration 17, loss = 0.20311896\n",
      "Iteration 18, loss = 0.20138322\n",
      "Iteration 19, loss = 0.19884633\n",
      "Iteration 20, loss = 0.19626397\n",
      "Iteration 1, loss = 0.56032742\n",
      "Iteration 2, loss = 0.28970362\n",
      "Iteration 3, loss = 0.24941093\n",
      "Iteration 4, loss = 0.23054159\n",
      "Iteration 5, loss = 0.22231629\n",
      "Iteration 6, loss = 0.20965695\n",
      "Iteration 7, loss = 0.20548138\n",
      "Iteration 8, loss = 0.20116879\n",
      "Iteration 9, loss = 0.19466983\n",
      "Iteration 10, loss = 0.19458784\n",
      "Iteration 11, loss = 0.19005269\n",
      "Iteration 12, loss = 0.18761464\n",
      "Iteration 13, loss = 0.18344672\n",
      "Iteration 14, loss = 0.18166413\n",
      "Iteration 15, loss = 0.18059951\n",
      "Iteration 16, loss = 0.17732543\n",
      "Iteration 17, loss = 0.17354898\n",
      "Iteration 18, loss = 0.17192863\n",
      "Iteration 19, loss = 0.17332580\n",
      "Iteration 20, loss = 0.17249989\n",
      "Iteration 1, loss = 0.57302990\n",
      "Iteration 2, loss = 0.29656400\n",
      "Iteration 3, loss = 0.26698807\n",
      "Iteration 4, loss = 0.24655407\n",
      "Iteration 5, loss = 0.23715516\n",
      "Iteration 6, loss = 0.22941478\n",
      "Iteration 7, loss = 0.21896675\n",
      "Iteration 8, loss = 0.21594348\n",
      "Iteration 9, loss = 0.20590660\n",
      "Iteration 10, loss = 0.20439452\n",
      "Iteration 11, loss = 0.20291431\n",
      "Iteration 12, loss = 0.19800824\n",
      "Iteration 13, loss = 0.19549068\n",
      "Iteration 14, loss = 0.19377665\n",
      "Iteration 15, loss = 0.19429252\n",
      "Iteration 16, loss = 0.18807027\n",
      "Iteration 17, loss = 0.18531082\n",
      "Iteration 18, loss = 0.18766496\n",
      "Iteration 19, loss = 0.18566379\n",
      "Iteration 20, loss = 0.18471197\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.55278153\n",
      "Iteration 2, loss = 0.28113118\n",
      "Iteration 3, loss = 0.25236253\n",
      "Iteration 4, loss = 0.23820928\n",
      "Iteration 5, loss = 0.28619170\n",
      "Iteration 6, loss = 0.22292058\n",
      "Iteration 7, loss = 0.63944969\n",
      "Iteration 8, loss = 0.35482306\n",
      "Iteration 9, loss = 0.47275501\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.56110634\n",
      "Iteration 2, loss = 0.27086187\n",
      "Iteration 3, loss = 0.33354375\n",
      "Iteration 4, loss = 0.24665554\n",
      "Iteration 5, loss = 0.24153247\n",
      "Iteration 6, loss = 0.25716592\n",
      "Iteration 7, loss = 0.21902377\n",
      "Iteration 8, loss = 0.21008423\n",
      "Iteration 9, loss = 0.20409173\n",
      "Iteration 10, loss = 0.22549260\n",
      "Iteration 11, loss = 0.19479912\n",
      "Iteration 12, loss = 0.19303428\n",
      "Iteration 13, loss = 0.19012974\n",
      "Iteration 14, loss = 0.18655673\n",
      "Iteration 15, loss = 0.18256260\n",
      "Iteration 16, loss = 0.18233091\n",
      "Iteration 17, loss = 0.22705545\n",
      "Iteration 18, loss = 0.18387029\n",
      "Iteration 19, loss = 0.18199129\n",
      "Iteration 20, loss = 0.18717025\n",
      "Iteration 1, loss = 0.34126923\n",
      "Iteration 2, loss = 0.14106596\n",
      "Iteration 3, loss = 0.10863100\n",
      "Iteration 4, loss = 0.09049870\n",
      "Iteration 5, loss = 0.07458603\n",
      "Iteration 6, loss = 0.06383651\n",
      "Iteration 7, loss = 0.05489282\n",
      "Iteration 8, loss = 0.05160220\n",
      "Iteration 9, loss = 0.04578838\n",
      "Iteration 10, loss = 0.03919495\n",
      "Iteration 11, loss = 0.03759845\n",
      "Iteration 12, loss = 0.03542775\n",
      "Iteration 13, loss = 0.02951086\n",
      "Iteration 14, loss = 0.03146109\n",
      "Iteration 15, loss = 0.02859219\n",
      "Iteration 16, loss = 0.02749276\n",
      "Iteration 17, loss = 0.02228806\n",
      "Iteration 18, loss = 0.02271698\n",
      "Iteration 19, loss = 0.01866011\n",
      "Iteration 20, loss = 0.01845294\n",
      "Iteration 1, loss = 0.34491469\n",
      "Iteration 2, loss = 0.13675292\n",
      "Iteration 3, loss = 0.10274984\n",
      "Iteration 4, loss = 0.08136811\n",
      "Iteration 5, loss = 0.06933750\n",
      "Iteration 6, loss = 0.05940358\n",
      "Iteration 7, loss = 0.05087462\n",
      "Iteration 8, loss = 0.04566974\n",
      "Iteration 9, loss = 0.04127861\n",
      "Iteration 10, loss = 0.03738845\n",
      "Iteration 11, loss = 0.03710707\n",
      "Iteration 12, loss = 0.03048123\n",
      "Iteration 13, loss = 0.02642311\n",
      "Iteration 14, loss = 0.02802942\n",
      "Iteration 15, loss = 0.02611715\n",
      "Iteration 16, loss = 0.02080488\n",
      "Iteration 17, loss = 0.02404455\n",
      "Iteration 18, loss = 0.02100228\n",
      "Iteration 19, loss = 0.01823479\n",
      "Iteration 20, loss = 0.02130245\n",
      "Iteration 1, loss = 0.35451881\n",
      "Iteration 2, loss = 0.13669896\n",
      "Iteration 3, loss = 0.10404620\n",
      "Iteration 4, loss = 0.08410747\n",
      "Iteration 5, loss = 0.07318854\n",
      "Iteration 6, loss = 0.06248125\n",
      "Iteration 7, loss = 0.05561420\n",
      "Iteration 8, loss = 0.05005054\n",
      "Iteration 9, loss = 0.04433262\n",
      "Iteration 10, loss = 0.03982166\n",
      "Iteration 11, loss = 0.03304217\n",
      "Iteration 12, loss = 0.03205780\n",
      "Iteration 13, loss = 0.02908183\n",
      "Iteration 14, loss = 0.02722036\n",
      "Iteration 15, loss = 0.02657835\n",
      "Iteration 16, loss = 0.02593097\n",
      "Iteration 17, loss = 0.02538093\n",
      "Iteration 18, loss = 0.02375208\n",
      "Iteration 19, loss = 0.01872001\n",
      "Iteration 20, loss = 0.02053213\n",
      "Iteration 1, loss = 0.33951772\n",
      "Iteration 2, loss = 0.13727194\n",
      "Iteration 3, loss = 0.10293730\n",
      "Iteration 4, loss = 0.08773452\n",
      "Iteration 5, loss = 0.07095824\n",
      "Iteration 6, loss = 0.06180785\n",
      "Iteration 7, loss = 0.05520991\n",
      "Iteration 8, loss = 0.04901658\n",
      "Iteration 9, loss = 0.04419018\n",
      "Iteration 10, loss = 0.04029643\n",
      "Iteration 11, loss = 0.03734581\n",
      "Iteration 12, loss = 0.02945566\n",
      "Iteration 13, loss = 0.02835677\n",
      "Iteration 14, loss = 0.02600738\n",
      "Iteration 15, loss = 0.02334003\n",
      "Iteration 16, loss = 0.19019276\n",
      "Iteration 17, loss = 0.07249722\n",
      "Iteration 18, loss = 0.05755432\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.34236467\n",
      "Iteration 2, loss = 0.13933641\n",
      "Iteration 3, loss = 0.10772213\n",
      "Iteration 4, loss = 0.09363879\n",
      "Iteration 5, loss = 0.07168426\n",
      "Iteration 6, loss = 0.06125527\n",
      "Iteration 7, loss = 0.05644705\n",
      "Iteration 8, loss = 0.04697108\n",
      "Iteration 9, loss = 0.04298009\n",
      "Iteration 10, loss = 0.03807363\n",
      "Iteration 11, loss = 0.03186969\n",
      "Iteration 12, loss = 0.03197412\n",
      "Iteration 13, loss = 0.07019967\n",
      "Iteration 14, loss = 0.03795037\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.32075588\n",
      "Iteration 2, loss = 0.14371505\n",
      "Iteration 3, loss = 0.10497255\n",
      "Iteration 4, loss = 0.08706503\n",
      "Iteration 5, loss = 0.07425768\n",
      "Iteration 6, loss = 0.06405363\n",
      "Iteration 7, loss = 0.05640603\n",
      "Iteration 8, loss = 0.05108043\n",
      "Iteration 9, loss = 0.04559623\n",
      "Iteration 10, loss = 0.04158197\n",
      "Iteration 11, loss = 0.03505335\n",
      "Iteration 12, loss = 0.03281433\n",
      "Iteration 13, loss = 0.02868126\n",
      "Iteration 14, loss = 0.02571878\n",
      "Iteration 15, loss = 0.02355611\n",
      "Iteration 16, loss = 0.02069391\n",
      "Iteration 17, loss = 0.01820300\n",
      "Iteration 18, loss = 0.01576821\n",
      "Iteration 19, loss = 0.01395867\n",
      "Iteration 20, loss = 0.01334809\n",
      "\n",
      "RESULTS FOR NN\n",
      "\n",
      "Best parameters set found:\n",
      "{'hidden_layer_sizes': (50,)}\n",
      "Score with best parameters:\n",
      "0.970833333333\n",
      "\n",
      "All scores on the grid:\n",
      "[ 0.91263333  0.97083333  0.92371667  0.96851667]\n"
     ]
    }
   ],
   "source": [
    "# for NN we try the same architectures as before\n",
    "# Since it takes too long for all architectures, I remove two of them.\n",
    "parameters_nn_large = {'max_iter': 20, 'alpha': 1e-4, 'hidden_layer_sizes': [(50,), (10,10,)], \n",
    "                       'solver': 'sgd', 'tol': 1e-4, 'random_state': ID, 'learning_rate_init': .1, 'verbose': True}\n",
    "mlp_large = MLPClassifier(**parameters_nn_large)\n",
    "\n",
    "mlp_large_CV = GridSearchCV(mlp, parameters, cv=5)\n",
    "mlp_large_CV.fit(X_train, y_train) \n",
    "\n",
    "print ('\\nRESULTS FOR NN\\n')\n",
    "print(\"Best parameters set found:\")\n",
    "print(mlp_large_CV.best_params_)\n",
    "print(\"Score with best parameters:\")\n",
    "print(mlp_large_CV.best_score_)\n",
    "print(\"\\nAll scores on the grid:\")\n",
    "print(mlp_large_CV.cv_results_['mean_test_score'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now get training and test error for a NN with best parameters from above. Use verbose=True\n",
    "in input so to see how loss changes in iterations (see how this changs if the number of iterations is changed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.32075588\n",
      "Iteration 2, loss = 0.14371505\n",
      "Iteration 3, loss = 0.10497255\n",
      "Iteration 4, loss = 0.08706503\n",
      "Iteration 5, loss = 0.07425768\n",
      "Iteration 6, loss = 0.06405363\n",
      "Iteration 7, loss = 0.05640603\n",
      "Iteration 8, loss = 0.05108043\n",
      "Iteration 9, loss = 0.04559623\n",
      "Iteration 10, loss = 0.04158197\n",
      "Iteration 11, loss = 0.03505335\n",
      "Iteration 12, loss = 0.03281433\n",
      "Iteration 13, loss = 0.02868126\n",
      "Iteration 14, loss = 0.02571878\n",
      "Iteration 15, loss = 0.02355611\n",
      "Iteration 16, loss = 0.02069391\n",
      "Iteration 17, loss = 0.01820300\n",
      "Iteration 18, loss = 0.01576821\n",
      "Iteration 19, loss = 0.01395867\n",
      "Iteration 20, loss = 0.01334809\n",
      "\n",
      "RESULTS FOR BEST NN\n",
      "\n",
      "Best NN training error: 0.002067\n",
      "Best NN test error: 0.025700\n"
     ]
    }
   ],
   "source": [
    "# get training and test error for the best NN model from CV\n",
    "best_parameters_mpl_large = {'max_iter': 20, 'alpha': 1e-4, 'solver': 'sgd', 'tol': 1e-4, \n",
    "                             'random_state': ID, 'learning_rate_init': .1, 'verbose': True}\n",
    "best_parameters_mpl_large['hidden_layer_sizes'] = mlp_large_CV.best_params_['hidden_layer_sizes']\n",
    "best_mlp_large = MLPClassifier(**best_parameters_mpl_large)\n",
    "best_mlp_large.fit(X_train, y_train)\n",
    "\n",
    "training_error = 1. - best_mlp_large.score(X_train, y_train)\n",
    "test_error = 1. - best_mlp_large.score(X_test, y_test)\n",
    "\n",
    "print ('\\nRESULTS FOR BEST NN\\n')\n",
    "print (\"Best NN training error: %f\" % training_error)\n",
    "print (\"Best NN test error: %f\" % test_error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TO DO 8 \n",
    "\n",
    "Compare and discuss:\n",
    "- the results from standard SVM and NN with m=60000 training data points. If you stopped the standard SVM (because it was running for too long), include such aspect in your comparison. Compare also with the results of logistic regression as well.\n",
    "- the results from NN with m=500 and m=60000 training data points.\n",
    "\n",
    "### ANSWER\n",
    "First of all, I limited the number of iteration of SVM at 50 and reduced the number of architectures for CV of NN at two, otherwise the code it would take a really long time to run. With 50 iterations the block of code regarding SVM runs in at least 5 minutes on my PC. Logistic regression takes 5 minutes as well. The CV for NN with two architectures takes 5 minutes.\n",
    "\n",
    "The results above tell us that NN with 60000 training points score better than the standard SVM (and logistic regression), both on training and test error. Also, LR performs slightly better than SVM. As said before, since I set an upper bound on the number of iterations, it is possible that SVM did not converge to the optimal solution. \n",
    "\n",
    "Both NN for 500 and 60000 training data points achieve better results than regular SVM. Coincidentally, the architecture is the same, however two architectures were not available for CV for NN with 60000 data points. The NN with 60000 data points achieves a better test error than the NN with 500 data points. Since the training error is the same for both NNs, the test error could be influenced by the fact that the test set for the first NN is really large compared to the one for the second NN and so it could be more likely that more points are missclassified (or it would just perform worse)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TO DO 9 \n",
    "Plot (using function $plot\\_digit$ from above)  a digit that is missclassified by SVM or logistic regression and correctly classified by NN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INPUT:\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f306d456410>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LABEL: 9\n",
      "('Logistic regression prediction: ', 4.0)\n",
      "('NN prediction: ', 9.0)\n"
     ]
    }
   ],
   "source": [
    "if svm_or_lr['svm'] > svm_or_lr['lr']:\n",
    "    lr_svm_prediction = best_SVM_large.predict(X_test)\n",
    "else:\n",
    "    lr_svm_prediction = logistic_regression_large.predict(X_test)\n",
    "large_NN_prediction = best_mlp_large.predict(X_test)\n",
    "i = 0\n",
    "found = False\n",
    "while ((not found) and (i<len(y_test))):\n",
    "    if (lr_svm_prediction[i] != y_test[i]) and (large_NN_prediction[i] == y_test[i]):\n",
    "        plot_digit(X_test, y_test, i)\n",
    "        print(\"Logistic regression prediction: \", lr_svm_prediction[i])\n",
    "        print(\"NN prediction: \", large_NN_prediction[i])\n",
    "        found = True\n",
    "    else:\n",
    "        i += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TO DO 10\n",
    "Plot (using function $plot\\_digit$ from above)  a digit that was missclassified by NN with m=500 training data points and it is now instead correctly classified by NN with m=60000 training data points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INPUT:\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f306014db10>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LABEL: 8\n",
      "('NN prediction with training set size of 500: ', 3.0)\n",
      "('NN prediction with training set size of 60000: ', 8.0)\n"
     ]
    }
   ],
   "source": [
    "NN_prediction = mlp.predict(X_test)\n",
    "i = 0\n",
    "found = False\n",
    "while ((not found) and (i<len(y_test))):\n",
    "    if (NN_prediction[i] != y_test[i]) and (large_NN_prediction[i] == y_test[i]):\n",
    "        plot_digit(X_test, y_test, i)\n",
    "        print(\"NN prediction with training set size of 500: \", NN_prediction[i])\n",
    "        print(\"NN prediction with training set size of 60000: \", large_NN_prediction[i])\n",
    "        found = True\n",
    "    else:\n",
    "        i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
