{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Regression on House Pricing Dataset: Variable Selection & Regularization\n",
    "We consider a reduced version of a dataset containing house sale prices for King County, which includes Seattle. It includes homes sold between May 2014 and May 2015.\n",
    "\n",
    "[https://www.kaggle.com/harlfoxem/housesalesprediction]\n",
    "\n",
    "For each house we know 19 house features (e.g., number of bedrooms, number of bathrooms, etc.) plus its price, that is what we would like to predict."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TO DO: insert your ID number (\"numero di matricola\") below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "#put here your ``numero di matricola''\n",
    "numero_di_matricola =  1110975"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#import all packages needed\n",
    "%matplotlib inline\n",
    "from __future__ import division\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the data, remove data samples/points with missing values (NaN) and take a look at them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load the data\n",
    "df = pd.read_csv('kc_house_data.csv', sep = ',')\n",
    "\n",
    "#remove the data samples with missing values (NaN)\n",
    "df = df.dropna() \n",
    "\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract input and output data. We want to predict the price by using othr features (other than id) as input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Data = df.values\n",
    "# m = number of input samples\n",
    "m = 3164\n",
    "Y = Data[:m,2]\n",
    "X = Data[:m,3:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Pre-Processing\n",
    "\n",
    "Split the data into training  set of $m_{train}$ samples, validation set of $m_{val}$ samples and a test set of $m_{test}:=m-m_{train}-m_{val}$ samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into train (50 samples) and test data (the rest)\n",
    "m_train = 50\n",
    "\n",
    "m_test = m - m_train \n",
    "from sklearn.cross_validation import train_test_split\n",
    "\n",
    "XtrainOLS, Xtest, YtrainOLS, Ytest = train_test_split(X, Y, test_size=m_test/m, random_state=numero_di_matricola)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Standardize the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data pre-processing\n",
    "from sklearn import preprocessing\n",
    "scaler = preprocessing.StandardScaler().fit(XtrainOLS)\n",
    "XtrainOLS = scaler.transform(XtrainOLS)\n",
    "\n",
    "Xtest = scaler.transform(Xtest)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Least-Squares Solution\n",
    "\n",
    "The routine LinearRegression.score(X,y) computes the *Coefficient of determination* $R^2$, defined as:\n",
    "\n",
    "$$R^2 = 1- \\frac{RSS}{TSS}$$\n",
    "\n",
    "where $RSS$ is the *Residual Sum of Squares* and $TSS$ is the *Total Sum of Square*. Denoting with $\\hat{y}_i$ the $i$-th predicted output values, they are so defined:\n",
    "\n",
    "\\begin{align*}\n",
    "RSS &= \\sum_{i=1}^m (y_i - \\hat{y}_i)^2\\\\\n",
    "TSS &= \\sum_{i=1}^m (y_i -\\bar{y}_i)^2, \\qquad \\qquad \\bar{y}_i=\\frac{1}{m} \\sum_{i=1}^m y_i\n",
    "\\end{align*}\n",
    "\n",
    "In this notebook we will mostly use the coefficient of determination $R^2$ (instead of the RSS) as a measure to compare models and choose tuning parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO 1\n",
    "\n",
    "Answer the following: are we interested  in models with low $R^2$ or high $R^2$? Why? (max 5 lines)\n",
    "\n",
    "#### COMPLETE WITH ANSWER:\n",
    "We are interested in models with high $R^2$: the closer the coefficient of determination is to 1 the better linear regression fits the data with respect to the average. The RSS represents the deviation of the predictions with respect to the true value and the TSS represents the deviation of the mean value to the true value: better predictions (in comparison to the simple average value) achive an RSS higher than TSS, so the fraction between the two is less than 1 and, ideally, goes to 0 (i.e., RSS $= 0$ means that each prediction is equal to the true value). It is possibile to obatin $R^2 < 0$: in that case the model found performs worse than the predictions based on the average.\n",
    " \n",
    "\n",
    "\n",
    "\n",
    "Now compute the Least-Squares estimate using LinearRegression() in Scikit-learn, and print the corresponding score in training and test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Least-Squares\n",
    "from sklearn import linear_model \n",
    "#OLS is the linear regression model\n",
    "OLS = linear_model.LinearRegression()\n",
    "\n",
    "#fit the model on training data\n",
    "OLS.fit(XtrainOLS, YtrainOLS)\n",
    "\n",
    "#obtain predictions on training data\n",
    "Yhat_tr = OLS.predict(XtrainOLS)\n",
    "\n",
    "\n",
    "#coefficients from the model\n",
    "b_LS = np.hstack((OLS.intercept_, OLS.coef_))\n",
    "\n",
    "print \"Coefficient of determination on training data:\", OLS.score(XtrainOLS,YtrainOLS)\n",
    "print \"Coefficient of determination on test data:\", OLS.score(Xtest,Ytest)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confidence Intervals\n",
    "\n",
    "We now compute the confidence interval for each coefficient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Least-Squares: Confidence Intervals\n",
    "from scipy.stats import t\n",
    "\n",
    "Xtrain_im_testrcept = np.hstack((np.ones((XtrainOLS.shape[0],1)), XtrainOLS))\n",
    "\n",
    "#alpha for confidence intervals\n",
    "alpha = 0.05\n",
    "\n",
    "#quantile from t-student distribution\n",
    "tperc = t.ppf(1-alpha/2, m_train-XtrainOLS.shape[1]-1, loc=0, scale=1)\n",
    "sigma2 = np.linalg.norm(YtrainOLS-Yhat_tr)**2/(m_train-XtrainOLS.shape[1]-1)\n",
    "\n",
    "R = np.dot(Xtrain_im_testrcept.transpose(),Xtrain_im_testrcept)\n",
    "Ur, Sr, Vr = np.linalg.svd(R, full_matrices=1, compute_uv=1)\n",
    "\n",
    "\n",
    "Sri = (1/Sr)*(Sr>0)\n",
    "Sri = Sri*(Sri<1e10)\n",
    "\n",
    "print Sri\n",
    "\n",
    "Ri2 = np.dot(Ur,np.dot(np.diag(Sri),np.transpose(Ur)))\n",
    "\n",
    "Ri = np.linalg.pinv(R)\n",
    "\n",
    "# TO AVOID LATER COMPUTATION OF SQRT OF NEGATIVE VALUES\n",
    "# Ri_diag = np.diag(Ri).copy()\n",
    "# Ri_diag[np.where(Rid<0)] = 0\n",
    "# print Ri_diag\n",
    "print np.diag(Ri)\n",
    "print np.diag(Ri2)\n",
    "\n",
    "# TO AVOID COMPUTATION OF SQRT OF NEGATIVE VALUES\n",
    "# v = np.sqrt(Ri_diag)\n",
    "v = np.sqrt(np.diag(Ri))\n",
    "Delta = np.sqrt(sigma2)*v*tperc\n",
    "CI = np.transpose(np.vstack((b_LS,b_LS))) + np.transpose(np.vstack((-Delta,+Delta) ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the LS coefficients and their confidence im_testrval."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot confidence im_testrvals\n",
    "plt.figure(1)\n",
    "plt.plot(b_LS[1:], 'r', marker='o', ms=7.0)\n",
    "plt.plot(CI[1:,0], 'b--')\n",
    "plt.plot(CI[1:,1], 'b--')\n",
    "plt.plot(np.zeros(b_LS.shape[0],), 'k', linewidth=2.0)\n",
    "plt.xlabel('Coefficient Index')\n",
    "plt.ylabel('LS Coefficient')\n",
    "plt.title('Coefficients and Confidence Sets')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question: based on the results above, if you had to choose at most 5 features for a linear regression model, which ones would you choose? Why?\n",
    "\n",
    "### TODO 3\n",
    "Answer the question above (max 5 lines)\n",
    "\n",
    "If I had to choose at most 5 features for a linear regression model, based on the results above, I would choose the coefficients that have the highest (absolute) value, thus discarding those closer to 0 since they do not add that much to the model. In this case I can do that because we first normalized the data, so we do not bump into scaling problems. In general we have to take into account scaling problems and also correlations between features before selecting only some of them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Best-Subset Selection\n",
    "\n",
    "Splitting the OLS training data into a training data and validation dataset perform best-subset selection. \n",
    "\n",
    "For $k$ going from 1 to $n_{sub}=4$:\n",
    "1. Compute the LS estimate using all the possible subsets of $k$ features\n",
    "2. Compute the prediction error on the validation dataset\n",
    "\n",
    "Finally we choose the subset of $k^*$ features givein the lowest validation error.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import itertools\n",
    "import math \n",
    "\n",
    "m_trainBSS=int(math.ceil(m_train/2))\n",
    "m_valBSS=m_train-m_trainBSS\n",
    "\n",
    "\n",
    "Xtrain = XtrainOLS[:m_trainBSS,:]\n",
    "Ytrain = YtrainOLS[:m_trainBSS]\n",
    "Xval = XtrainOLS[m_trainBSS:,:]\n",
    "Yval = YtrainOLS[m_trainBSS:,]\n",
    "\n",
    "\n",
    "nsub = 4\n",
    "features_idx_dict = {}\n",
    "validation_err_dict = {}\n",
    "validation_err_min = np.zeros(nsub,)\n",
    "validation_err_min_idx = np.zeros(nsub, dtype=np.int64)\n",
    "for k in range(1,nsub+1):\n",
    "    features_idx = list(itertools.combinations(range(Xtrain.shape[1]),k))\n",
    "    validation_error = np.zeros(len(features_idx),)\n",
    "    for j in range(len(features_idx)):\n",
    "        OLS_subset = linear_model.LinearRegression()\n",
    "        OLS_subset.fit(Xtrain[:,features_idx[j]], Ytrain)\n",
    "        validation_error[j] = 1 - OLS_subset.score(Xval[:,features_idx[j]], Yval)\n",
    "        validation_error[j] = 1 - OLS_subset.score(Xtest[:,features_idx[j]], Ytest)\n",
    "    validation_err_min[k-1] = np.min(validation_error)    \n",
    "    validation_err_min_idx[k-1] = np.argmin(validation_error)\n",
    "    features_idx_dict.update({k: features_idx})\n",
    "    validation_err_dict.update({k: validation_error})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the validation error as a function of the number of retained features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot\n",
    "plt.figure(2)\n",
    "for k in range(1,nsub+1):\n",
    "    plt.scatter(k*np.ones(validation_err_dict[k].shape), validation_err_dict[k], color='k', alpha=0.5)\n",
    "    #plt.scatter(k, validation_err_min[k-1], color='r', alpha=0.8)\n",
    "    if k > 1:\n",
    "        plt.plot([k-1, k], [validation_err_min[k-2], validation_err_min[k-1]], color='r',marker='o', \n",
    "            markeredgecolor='k', markerfacecolor = 'r', markersize = 10)\n",
    "plt.xlabel('Number of retained features')\n",
    "plt.ylabel('RSS/TSS')\n",
    "plt.title('Best-Subset Selection')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute the LS estimate using the selected subset of features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO 4: pick the number of features for the best subset according to figure above, select the best subset using the results above and learn the model on the entire training data; finally compute score on training and on test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OLS_best_subset = linear_model.LinearRegression()\n",
    "\n",
    "# now pick the number of features according to best subset\n",
    "opt_num_features = np.argmin(validation_err_min)\n",
    "\n",
    "#opt_features_idx contains the indices of the features from best subset\n",
    "opt_features_idx = features_idx_dict[opt_num_features][validation_err_min_idx[opt_num_features-1]]\n",
    "\n",
    "\n",
    "#let's print the indices of the features from best subset\n",
    "print opt_features_idx\n",
    "\n",
    "#fit the best subset on the entire training set\n",
    "OLS_best_subset.fit(XtrainOLS[:,opt_features_idx], YtrainOLS)\n",
    "\n",
    "#print the coefficient of determination on training and on test data\n",
    "print \"Coefficient of determination on training data:\", OLS_best_subset.score(XtrainOLS[:,opt_features_idx],YtrainOLS)\n",
    "print \"Coefficient of determination on test data:\", OLS_best_subset.score(Xtest[:,opt_features_idx],Ytest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO 5: do the features from best subset selection correspond to the ones you would have chosen based on confidence im_testrvals for the linear regression coefficients? Comment (max 5 lines)\n",
    "\n",
    "### ANSWER\n",
    "No, not entirely, I would not have chosen features number 11, for example, since it has a coefficient with a very low (absolute) value. With 4 features, I would have chosen features 3, 8, 15 and 16 for the reason explained earlier. However, the features selected by Best-Subset Selection (square footage of the home, if the house has been viewed, the overall grade and the year that the house was build) are more meaningful in determining the price of the house than what I would have chosen looking only at value of the features instead of evalutaing also the meaning of such features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lasso\n",
    "\n",
    "### TO DO 6\n",
    "Use the routine *lasso_path* from *sklearn.linear_regression* to compute the \"lasso path\" for different values of the regularization parameter $\\lambda$. You should first fix a grid a possible values of lambda (the variable \"lasso_lams\"). For each entry of the vector \"lasso_lams\" you should compute the corresponding model (The i-th column of the vector  \"lasso_coefs\" should contain the coefficients of the linear model computed using lasso_lams[i] as regularization parameter).\n",
    "\n",
    "Be careful that the grid should be chosen appropriately.\n",
    "Note that the parameter $\\lambda$ is called $\\alpha$ in the Lasso model from sklearn.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import lasso_path\n",
    "\n",
    "# select a grid of possible regularization parameters \n",
    "# (be carefull how this is chosen, you may have to refine the choice after having seen the results)\n",
    "lasso_lams = np.arange(1000, 100000, 1000)\n",
    "\n",
    "\n",
    "# Use the function lasso_path to compute the \"lasso path\", passing in input the lambda values\n",
    "# you have specified in lasso_lams\n",
    "lasso_lams, lasso_coefs, _ = lasso_path(X, Y, alphas=lasso_lams)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate the sparsity in the estimated coefficients as a function of the regularization parameter $\\lambda$: to this purpose, compute the number of non-zero entries in the estimated coefficient vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l0_coef_norm = np.zeros(len(lasso_lams),)\n",
    "\n",
    "for i in range(len(lasso_lams)):\n",
    "    l0_coef_norm[i] = sum(lasso_coefs[:,i]!=0)\n",
    "\n",
    "\n",
    "plt.figure(6)\n",
    "plt.plot(lasso_lams, l0_coef_norm, marker='o', markersize=5)\n",
    "plt.xlabel('Lambda')\n",
    "plt.ylabel('Number of non-zero coefficients')\n",
    "plt.title('Sparsity Degree')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO 7: explain the results in the figure above (max 5 lines)\n",
    "\n",
    "### ANSWER\n",
    "In the figure above we can see how many coefficients are set to 0 on different values of lambda. We note that, in general, the higher the value of lambda is the higher is the number of coefficients set 0. However, the sparsity degree of the coefficients set to 0 with respect to the value of lambda clearly is not a monotonically decreasing function. With this information we can practically perform feature selection using lambda as a way to exclude (i.e., set to zero) features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO 8: Use k-fold Cross-Validation to fix the regularization parameter\n",
    "\n",
    "Use the scikit-learn built-in routine *Lasso* (from the *linear_regression* package) to compute the lasso  coefficients.\n",
    "\n",
    "Use *KFold* from *sklearn.cross_validation* to split the data i.e. XtrainOLS and YtrainOLS) into the desired number of folds.\n",
    "\n",
    "The pick $lam\\_opt$ to be the chosen value for the regularization parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cross_validation import KFold\n",
    "num_folds = 5\n",
    "\n",
    "kf = KFold(n=m_train, n_folds = num_folds)\n",
    "\n",
    "#loss_ridge_kfold will contain the value of the loss\n",
    "loss_lasso_kfold = np.zeros(len(lasso_lams),)\n",
    "\n",
    "for i in range(len(lasso_lams)):\n",
    "    \n",
    "    #define a lasso model  using linear_model.Lasso() for the i-th value of lam_values\n",
    "    lasso_kfold = linear_model.Lasso(alpha=lasso_lams[i]) \n",
    "    for train_index, validation_index in kf:\n",
    "        Xtrain_kfold, Xva_kfold = XtrainOLS[train_index], XtrainOLS[validation_index]\n",
    "        Ytrain_kfold, Yva_kfold = YtrainOLS[train_index], YtrainOLS[validation_index]\n",
    "        \n",
    "        #learn the model using the training data from the k-fold\n",
    "        lasso_kfold.fit(Xtrain_kfold, Ytrain_kfold)\n",
    "        \n",
    "        #compute the loss (loss_lasso_kfold) using the validation data from the k-fold\n",
    "        loss_lasso_kfold[i] = lasso_kfold.score(Xva_kfold, Yva_kfold)\n",
    "\n",
    "#choose the regularization parameter that minimizes the loss\n",
    "lasso_lam_opt =  lasso_lams[np.argmax(loss_lasso_kfold)]\n",
    "\n",
    "print \"Best value of the regularization parameter:\", lasso_lam_opt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the Cross-Validation estimate of the prediction error as a function of the regularization parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.figure(4)\n",
    "plt.xscale('log')\n",
    "plt.plot(lasso_lams, loss_lasso_kfold, color='b')\n",
    "plt.scatter(lasso_lams[np.argmin(loss_lasso_kfold)], loss_lasso_kfold[np.argmin(loss_lasso_kfold)], color='b', marker='o', linewidths=5)\n",
    "plt.xlabel('Lambda')\n",
    "plt.ylabel('Validation Error')\n",
    "plt.title('Lasso: choice of regularization parameter')\n",
    "plt.show()\n",
    "print \"Total number of coefficients:\", len(lasso_kfold.coef_)\n",
    "print \"Number of non-zero coefficients:\", sum(lasso_kfold.coef_ != 0)\n",
    "print \"Best value of regularization parameter:\", lasso_lam_opt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TO DO 9 now estimate the lasso coefficients using all the training data and the optimal regularization parameter (chosen at previous step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Estimate Lasso  Coefficients with all data (trainval) for the the optimal value lasso_lam_opt of the regularization paramter\n",
    "\n",
    "#define the model\n",
    "lasso_reg = linear_model.Lasso(alpha=lasso_lam_opt)\n",
    "\n",
    "#fit using the training data\n",
    "lasso_reg.fit(XtrainOLS, YtrainOLS)\n",
    "\n",
    "print \"Coefficient of determination on training data:\", lasso_reg.score(XtrainOLS,YtrainOLS)\n",
    "print \"Coefficient of determination on test data:\", lasso_reg.score(Xtest,Ytest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare the LS and the lasso  coefficients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare LS and lasso coefficients\n",
    "ind = np.arange(1,len(OLS.coef_)+1)  # the x locations for the groups\n",
    "width = 0.35       # the width of the bars\n",
    "fig, ax = plt.subplots()\n",
    "rects1 = ax.bar(ind, OLS.coef_, width, color='r')\n",
    "rects2 = ax.bar(ind + width, lasso_reg.coef_, width, color='y')\n",
    "ax.legend((rects1[0], rects2[0]), ('LS', 'Lasso'))\n",
    "plt.xlabel('Coefficient Idx')\n",
    "plt.ylabel('Coefficient Value')\n",
    "plt.title('LS and Lasso Coefficient')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print \"Coefficient of determination of LS on test data:\", OLS.score(Xtest,Ytest)\n",
    "print \"Coefficient of determination of LS (with subset selection) on test data:\", OLS_best_subset.score(Xtest[:,opt_features_idx],Ytest)\n",
    "print \"Coefficient of determination of LASSO on test data:\", lasso_reg.score(Xtest,Ytest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO 14: comment and compare the results obtained by the different methods (max 5 lines)\n",
    "\n",
    "### ANSWER\n",
    "We performed regularization with three different method, all of them based on the Least Squares method. The \"simple\" LS performs very poorly on test data. Least Square with Subset Selection performs better of all three, with very close results in both training and test data. LASSO is the best on training data but lose something on test data, overall still performing better of LS. The figure above compares coefficients of LS with coefficients of LASSO and we can see the shrinkage property of LASSO."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SUGGESTION (not compulsory): repeat the same as above use different data size, and try to understand the main differences\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate the performance on the test set\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
