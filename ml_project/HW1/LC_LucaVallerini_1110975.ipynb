{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification on Iris Dataset\n",
    "\n",
    "## IMPORTANT: make sure to rerun all the code from the beginning to obtain the results for the final version of your notebook, since this is the way we will do it before evaluting your notebook!!!\n",
    "\n",
    "### Dataset description\n",
    "\n",
    "We will be working with the famous “Iris” dataset that has been deposited on the UCI machine learning repository (https://archive.ics.uci.edu/ml/datasets/Iris).\n",
    "The iris dataset contains measurements for 150 iris flowers from three different species.\n",
    "\n",
    "The three classes in the Iris dataset:\n",
    "\n",
    "- Iris-setosa (n=50)\n",
    "\n",
    "- Iris-versicolor (n=50)\n",
    "\n",
    "- Iris-virginica (n=50)\n",
    "\n",
    "\n",
    "\n",
    "### Four features (regressors) are considered for the Iris dataset:\n",
    "\n",
    "\n",
    "\n",
    "1) sepal length in cm\n",
    "\n",
    "2) sepal width in cm\n",
    "\n",
    "3) petal length in cm\n",
    "\n",
    "4) petal width in cm\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first import all the packages that are needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "from scipy import stats\n",
    "from sklearn import datasets\n",
    "from sklearn import linear_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Perceptron\n",
    "We will implement the perceptron and use it to learn a halfspace."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TO DO** Set the random seed to your ID (matricola)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "IDnumber = 1110975\n",
    "np.random.seed(IDnumber)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the dataset from scikit learn and then split in training set and test set (50%-50%) after applying a random permutation to the datset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load the dataset from scikit learn\n",
    "iris = datasets.load_iris()\n",
    "m = iris.data.shape[0]\n",
    "permutation = np.random.permutation(m)\n",
    "X, Y = iris.data[permutation], iris.target[permutation]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to classify class \"1\" vs the other two classes (0 and 2). We are going to relabel the other classes (0 and 2) as \"-1\" so that we can use it directly with the perceptron."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#let's relabel classes 0 and 2 as -1\n",
    "for i in range(len(Y)):\n",
    "    if Y[i] != 1:\n",
    "        Y[i] = -1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TO DO** Divide the data into training set and test set (50% of the data each)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Divide in training and test: make sure that your training set\n",
    "#contains at least 10 elements from class 1 and at least 10 elements\n",
    "#from class -1! If it does not, modify the code so to apply more random\n",
    "#permutations (or the same permutation multiple times) until this happens.\n",
    "#IMPORTANT: do not change the random seed.\n",
    "\n",
    "#m_training needs to be the number of samples in the training set\n",
    "m_training = m/2\n",
    "\n",
    "#m_test needs to be the number of samples in the test set\n",
    "m_test = m/2\n",
    "\n",
    "# Since the permutation is random, I just split the samples in the middle.\n",
    "# The while loop assures that the condition that there are at least 10\n",
    "# elements from class 1  and at least 10 elements from class -1 in the \n",
    "# training set is met.\n",
    "while True:\n",
    "    # Instances for training set\n",
    "    X_training = X[m_training:]\n",
    "    # Labels for the training set\n",
    "    Y_training = Y[m_training:]\n",
    "    # Instances for test set\n",
    "    X_test = X[:m_test]\n",
    "    # Labels for the test set\n",
    "    Y_test = Y[:m_test]\n",
    "    \n",
    "    # Check the condition counting the elements for each class\n",
    "    count1, countm1 = 0, 0\n",
    "    for i in range(Y_test.shape[0]):\n",
    "        if count1 < 10 or countm1 < 10:\n",
    "            if Y_test[i] == 1:\n",
    "                count1 += 1\n",
    "            else:\n",
    "                countm1 += 1\n",
    "        else:\n",
    "            break # exit the loop as soon as the two lower bounds are met\n",
    "            \n",
    "    # if the conditions are met exit the while loop, otherwise keep trying\n",
    "    if count1 >= 10 and countm1 >= 10:\n",
    "        break\n",
    "    else:\n",
    "        permutation = np.random.permutation(m)\n",
    "        X, Y = iris.data[permutation], iris.target[permutation]\n",
    "        for i in range(len(Y)):\n",
    "            if Y[i] != 1:\n",
    "                Y[i] = -1\n",
    "\n",
    "        \n",
    "print Y_training #to make sure that Y_training contains both 1 and -1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TO DO** Now add a 1 in front of each sample so that we can use a vector to describe all the coefficients of the model. You can use the function $hstack$ in $numpy$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#add a 1 to each sample\n",
    "X_training = np.insert(X_training, 0, 1, axis=1)\n",
    "X_test = np.insert(X_test, 0, 1, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TO DO** Now complete the function *perceptron*. Since the perceptron does not terminate if the data is not linearly separable, your implementation should return the desired output (see below) if it reached the termination condition seen in class or if a maximum number of iterations have already been run, where 1 iteration corresponds to 1 update of the perceptron weights. In case the termination is reached because the maximum number of iterations have been completed, the implementation should return **the best model** seen up to now.\n",
    "\n",
    "The input parameters to pass are:\n",
    "- $X$: the matrix of input features, one row for each sample\n",
    "- $Y$: the vector of labels for the input features matrix X\n",
    "- $max\\_num\\_iterations$: the maximum number of iterations for running the perceptron\n",
    "\n",
    "The output values are:\n",
    "- $best\\_w$: the vector with the coefficients of the best model\n",
    "- $best\\_error$: the *fraction* of missclassified samples for the best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def perceptron(X, Y, max_num_iterations):\n",
    "    best_w = np.zeros(X.shape[1])\n",
    "    best_error = 0.\n",
    "    i = 0\n",
    "    while (i <= max_num_iterations):\n",
    "        for j in range(X.shape[0]):\n",
    "            if np.sign(np.inner(best_w, X[j])) != Y[j] and j <= X.shape[0]-1: # I found a mismatched sample\n",
    "                best_w = np.add(best_w, np.dot(Y[j], X[j]))\n",
    "                i += 1 # 1 iteration corresponds to 1 update\n",
    "                break;\n",
    "            elif j == X.shape[0]-1: # if there is no mismatched sample, il return the best model and the best_error\n",
    "                for k in range(X.shape[0]):\n",
    "                    if np.sign(np.inner(best_w, X[k])) != Y[k]:\n",
    "                        best_error += 1.\n",
    "                \n",
    "                return best_w, best_error\n",
    "    \n",
    "    for k in range(X.shape[0]):\n",
    "        if np.sign(np.inner(best_w, X[k])) != Y[k]:\n",
    "            best_error += 1.\n",
    "            \n",
    "    best_error /= X.shape[0]\n",
    "    \n",
    "    return best_w, best_error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we use the implementation above of the perceptron to learn a model from the training data using 100 iterations and print the error of the best model we have found."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#now run the perceptron for 100 iterations\n",
    "w_found, error = perceptron(X_training, Y_training, 100)\n",
    "print w_found, error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TO DO** use the best model $w\\_found$ to predict the labels for the test dataset and print the fraction of missclassified samples in the test set (that is an estimate of the true loss)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#now use the w_found to make predictions on test dataset\n",
    "\n",
    "num_errors = 0.\n",
    "for i in range(X_test.shape[0]):\n",
    "    if np.sign(np.inner(w_found, X_test[i])) != Y_test[i]:\n",
    "        num_errors += 1\n",
    "\n",
    "true_loss_estimate = num_errors/m_test\n",
    "#NOTE: you can avoid using num_errors if you prefer, as long as true_loss_estimate is correct\n",
    "print true_loss_estimate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TO DO** Copy the code from the last 2 cells above in the cell below and repeat the training with 10000 iterations. Then print the error in the training set and the estimate of the true loss obtained from the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#now run the perceptron for 10000 iterations here!\n",
    "\n",
    "w_found, error = perceptron(X_training, Y_training, 10000)\n",
    "\n",
    "print w_found, error\n",
    "\n",
    "num_errors = 0.\n",
    "\n",
    "for i in range(X_test.shape[0]):\n",
    "    if np.sign(np.inner(w_found, X_test[i])) != Y_test[i]:\n",
    "        num_errors += 1\n",
    "\n",
    "true_loss_estimate = num_errors/m_test\n",
    "print true_loss_estimate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TO DO** [Answer the following] What changes in the training error and in the test error (in terms of fraction of missclassified samples)? Explain what you observe. [Write the answer in this cell]\n",
    "\n",
    "**ANSWER**\n",
    "With more iteration we observe a (slightly) better training error, meaning that the perceptron had more \"time\" to find better coefficients. In terms of test error, the result is basically the same so, even tough the perceptron did find coefficients that works better on the training set, this improvement does not reflect on data that it has not seen yet, such as data in the test set. Nevertheless, we can say that the coefficients found are not that bad since the difference between the fraction of missclassified samples with respect to the training/test error is not that high."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression\n",
    "Now we use logistic regression, as implemented in Scikit-learn, to predict labels. We first do it for 2 labels and then for 3 labels. We will also plot the decision region of logistic regression.\n",
    "\n",
    "We first load the dataset again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load the dataset from scikit learn\n",
    "iris = datasets.load_iris()\n",
    "m = iris.data.shape[0]\n",
    "permutation = np.random.permutation(m)\n",
    "X, Y = iris.data[permutation], iris.target[permutation]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TO DO** As for the previous part, divide the data into training and test (50%-50%), relabel classes 0 and 2 as -1, and add a 1 as first component to each sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Divide in training and test: make sure that your training set\n",
    "#contains at least 10 elements from class 1 and at least 10 elements\n",
    "#from class -1! If it does not, modify the code so to apply more random\n",
    "#permutations (or the same permutation multiple times) until this happens.\n",
    "#IMPORTANT: do not change the random seed.\n",
    "\n",
    "m_training = m/2\n",
    "m_test = m/2\n",
    "\n",
    "#let's relabel classes 0 and 2 as -1\n",
    "for i in range(len(Y)):\n",
    "    if Y[i] != 1:\n",
    "        Y[i] = -1\n",
    "\n",
    "# Since the permutation is random, I just split the samples in the middle.\n",
    "# The while loop assures that the condition that there are at least 10\n",
    "# elements from class 1  and at least 10 elements from class -1 in the \n",
    "# training set is met.\n",
    "while True:\n",
    "    # Instances for training set\n",
    "    X_training = X[m_training:]\n",
    "    # Labels for the training set\n",
    "    Y_training = Y[m_training:]\n",
    "    # Instances for test set\n",
    "    X_test = X[:m_test]\n",
    "    # Labels for the test set\n",
    "    Y_test = Y[:m_test]\n",
    "    \n",
    "    # Check the condition counting the elements for each class\n",
    "    count1, countm1 = 0, 0\n",
    "    for i in range(Y_test.shape[0]):\n",
    "        if count1 < 10 or countm1 < 10:\n",
    "            if Y_test[i] == 1:\n",
    "                count1 += 1\n",
    "            else:\n",
    "                countm1 += 1\n",
    "        else:\n",
    "            break # exit the loop as soon as the two lower bounds are met\n",
    "            \n",
    "    # if the conditions are met exit the while loop, otherwise keep trying\n",
    "    if count1 >= 10 and countm1 >= 10:\n",
    "        break\n",
    "    else:\n",
    "        permutation = np.random.permutation(m)\n",
    "        X, Y = iris.data[permutation], iris.target[permutation]\n",
    "        for i in range(len(Y)):\n",
    "            if Y[i] != 1:\n",
    "                Y[i] = -1\n",
    "    \n",
    "#add a 1 to each sample\n",
    "X_training = np.insert(X_training, 0, 1, axis=1)\n",
    "X_test = np.insert(X_test, 0, 1, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To define a logistic regression model in Scikit-learn use the instruction\n",
    "\n",
    "$linear\\_model.LogisticRegression(C=1e5)$\n",
    "\n",
    "($C$ is a parameter related to *regularization*, a technique that\n",
    "we will see later in the course. Setting it to a high value is almost\n",
    "as ignoring regularization, so the instruction above corresponds to the\n",
    "logistic regression you have seen in class.)\n",
    "\n",
    "To learn the model you need to use the $fit(...)$ instruction and to predict you need to use the $predict(...)$ function. See the Scikit-learn documentation for how to use it.\n",
    "\n",
    "**TO DO** Define the logistic regression model, then learn the model using the training set and predict on the test set. Then print the fraction of samples missclassified in the training set and in the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#part on logistic regression for 2 classes\n",
    "logreg = linear_model.LogisticRegression(C=1e5)\n",
    "\n",
    "#learn from training set\n",
    "learning_training = logreg.fit(X_training, Y_training)\n",
    "\n",
    "#predict on training set\n",
    "prediction_training = logreg.predict(X_training)\n",
    "\n",
    "#print the error rate = fraction of missclassified samples\n",
    "error_rate_training = 0.\n",
    "for i in range(prediction_training.shape[0]):\n",
    "    if prediction_training[i] != Y_training[i]:\n",
    "        error_rate_training += 1.\n",
    "error_rate_training /= m_training\n",
    "print \"Error rate on training set: \"+str(error_rate_training)\n",
    "\n",
    "#predict on test set\n",
    "prediction_test = logreg.predict(X_test)\n",
    "\n",
    "#print the error rate = fraction of missclassified samples\n",
    "error_rate_test = 0.\n",
    "for i in range(prediction_test.shape[0]):\n",
    "    if prediction_test[i] != Y_test[i]:\n",
    "        error_rate_test += 1.\n",
    "error_rate_test /= m_test\n",
    "print \"Error rate on test set: \"+str(error_rate_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we do logistic regression for classification with 3 classes.\n",
    "\n",
    "**TO DO** First: let's load the data once again (with the same permutation from before)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#part on logistic regression for 3 classes\n",
    "\n",
    "#Divide in training and test: make sure that your training set\n",
    "#contains at least 10 elements from each of the 3 classes!\n",
    "#If it does not, modify the code so to apply more random\n",
    "#permutations (or the same permutation multiple times) until this happens.\n",
    "#IMPORTANT: do not change the random seed.\n",
    "X = iris.data[permutation]\n",
    "Y = iris.target[permutation]\n",
    "\n",
    "# Since the permutation is random, I just split the samples in the middle.\n",
    "# The while loop assures that the condition that there are at least 10\n",
    "# elements from each class is met.\n",
    "while True:\n",
    "    # Instances for training set\n",
    "    X_training = X[m_training:]\n",
    "    # Labels for the training set\n",
    "    Y_training = Y[m_training:]\n",
    "    # Instances for test set\n",
    "    X_test = X[:m_test]\n",
    "    # Labels for the test set\n",
    "    Y_test = Y[:m_test]\n",
    "    \n",
    "    # Check the condition counting the elements for each class\n",
    "    count0, count1, count2 = 0, 0, 0\n",
    "    for i in range(Y_test.shape[0]):\n",
    "        if count0 < 10 or count1 < 10 or count2 < 10:\n",
    "            if Y_test[i] == 0:\n",
    "                count0 += 1\n",
    "            elif Y_test[i] == 1:\n",
    "                count1 += 1\n",
    "            else:\n",
    "                count2 += 1\n",
    "        else:\n",
    "            break # exit the for loop as soon as the three lower bounds are met\n",
    "            \n",
    "    # if the conditions are met exit the while loop, otherwise keep trying\n",
    "    if count0 >= 10 and count1 >= 10 and count2 >= 10:\n",
    "        break\n",
    "    else:\n",
    "        permutation = np.random.permutation(m)\n",
    "        X, Y = iris.data[permutation], iris.target[permutation]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TO DO** Now perform logistic regression (instructions as before) for 3 classes, learning a model from the training set and predicting on the test set. Print the fraction of missclassified samples on the training set and the fraction of missclassified samples on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#part on logistic regression for 3 classes\n",
    "logreg = linear_model.LogisticRegression(C=1e5)\n",
    "\n",
    "#learn from training set\n",
    "learning_training = logreg.fit(X_training, Y_training)\n",
    "\n",
    "#predict on training set\n",
    "prediction_training = logreg.predict(X_training)\n",
    "\n",
    "#print the error rate = fraction of missclassified samples\n",
    "error_rate_training = 0.\n",
    "for i in range(prediction_training.shape[0]):\n",
    "    if prediction_training[i] != Y_training[i]:\n",
    "        error_rate_training += 1.\n",
    "error_rate_training /= m_training\n",
    "print \"Error rate on training set: \"+str(error_rate_training)\n",
    "\n",
    "#predict on test set\n",
    "prediction_test = logreg.predict(X_test)\n",
    "\n",
    "#print the error rate = fraction of missclassified samples\n",
    "error_rate_test = 0.\n",
    "for i in range(prediction_test.shape[0]):\n",
    "    if prediction_test[i] != Y_test[i]:\n",
    "        error_rate_test += 1.\n",
    "error_rate_test /= m_test\n",
    "print \"Error rate on test set: \"+str(error_rate_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TO DO** Now pick two features and restrict the dataset to include only two features, whose indices are specified in the $feature$ vector below. Then split into training and test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#to make the plot we need to reduce the data to 2D, so we choose two features\n",
    "features_list = ['sepal length', 'sepal width', 'petal length', 'petal width']\n",
    "labels_list = ['Iris-setosa', 'Iris-versicolor', 'Iris-virginica']\n",
    "\n",
    "index_feature1 = 0\n",
    "index_feature2 = 2\n",
    "features = [index_feature1, index_feature2]\n",
    "\n",
    "feature_name0 = features_list[features[0]]\n",
    "feature_name1 = features_list[features[1]]\n",
    "\n",
    "X = X[:,features]\n",
    "\n",
    "# Since the permutation is random, I just split the samples in the middle.\n",
    "# The while loop assures that the condition that there are at least 10\n",
    "# elements from each class is met.\n",
    "while True:\n",
    "    # Instances for training set\n",
    "    X_training = X[m_training:]\n",
    "    # Labels for the training set\n",
    "    Y_training = Y[m_training:]\n",
    "    # Instances for test set\n",
    "    X_test = X[:m_test]\n",
    "    # Labels for the test set\n",
    "    Y_test = Y[:m_test]\n",
    "    \n",
    "    # Check the condition counting the elements for each class\n",
    "    count0, count1, count2 = 0, 0, 0\n",
    "    for i in range(Y_test.shape[0]):\n",
    "        if count0 < 10 or count1 < 10 or count2 < 10:\n",
    "            if Y_test[i] == 0:\n",
    "                count0 += 1\n",
    "            elif Y_test[i] == 1:\n",
    "                count1 += 1\n",
    "            else:\n",
    "                count2 += 1\n",
    "        else:\n",
    "            break # exit the for loop as soon as the three lower bounds are met\n",
    "            \n",
    "    # if the conditions are met exit the while loop, otherwise keep trying\n",
    "    if count0 >= 10 and count1 >= 10 and count2 >= 10:\n",
    "        break\n",
    "    else:\n",
    "        permutation = np.random.permutation(m)\n",
    "        X, Y = iris.data[permutation], iris.target[permutation]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now learn a model using the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#part on logistic regression for three classes restricted to only two features\n",
    "logreg = linear_model.LogisticRegression(C=1e5)\n",
    "\n",
    "#learn from training set\n",
    "learning_training = logreg.fit(X_training, Y_training)\n",
    "\n",
    "#predict on training set\n",
    "prediction_training = logreg.predict(X_training)\n",
    "\n",
    "#print the error rate = fraction of missclassified samples\n",
    "error_rate_training = 0.\n",
    "for i in range(prediction_training.shape[0]):\n",
    "    if prediction_training[i] != Y_training[i]:\n",
    "        error_rate_training += 1.\n",
    "error_rate_training /= m_training\n",
    "print \"Error rate on training set: \"+str(error_rate_training)\n",
    "\n",
    "#predict on test set\n",
    "prediction_test = logreg.predict(X_test)\n",
    "\n",
    "#print the error rate = fraction of missclassified samples\n",
    "error_rate_test = 0.\n",
    "for i in range(prediction_test.shape[0]):\n",
    "    if prediction_test[i] != Y_test[i]:\n",
    "        error_rate_test += 1.\n",
    "error_rate_test /= m_test\n",
    "print \"Error rate on test set: \"+str(error_rate_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If everything is ok, the code below uses the model in $logreg$ to plot the decision region for the two features chosen above, with colors denoting the predicted value. It also plots the points (with correct labels) in the training set. It makes a similar plot for the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the decision boundary. For that, we will assign a color to each\n",
    "# point in the mesh [x_min, x_max]x[y_min, y_max].\n",
    "h = .02  # step size in the mesh\n",
    "x_min, x_max = X[:, 0].min() - .5, X[:, 0].max() + .5\n",
    "y_min, y_max = X[:, 1].min() - .5, X[:, 1].max() + .5\n",
    "xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n",
    "\n",
    "Z = logreg.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "\n",
    "# Put the result into a color plot\n",
    "Z = Z.reshape(xx.shape)\n",
    "plt.figure(1, figsize=(4, 3))\n",
    "plt.pcolormesh(xx, yy, Z, cmap=plt.cm.Paired)\n",
    "\n",
    "# Plot also the training points\n",
    "plt.scatter(X_training[:, 0], X_training[:, 1], c=Y_training, edgecolors='k', cmap=plt.cm.Paired)\n",
    "plt.xlabel(feature_name0)\n",
    "plt.ylabel(feature_name1)\n",
    "\n",
    "plt.xlim(xx.min(), xx.max())\n",
    "plt.ylim(yy.min(), yy.max())\n",
    "plt.xticks(())\n",
    "plt.yticks(())\n",
    "plt.title('Training set')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# Put the result into a color plot\n",
    "Z = Z.reshape(xx.shape)\n",
    "plt.figure(1, figsize=(4, 3))\n",
    "plt.pcolormesh(xx, yy, Z, cmap=plt.cm.Paired)\n",
    "\n",
    "# Plot also the test points \n",
    "plt.scatter(X_test[:, 0], X_test[:, 1], c=Y_test, edgecolors='k', cmap=plt.cm.Paired, marker='s')\n",
    "plt.xlabel(feature_name0)\n",
    "plt.ylabel(feature_name1)\n",
    "\n",
    "plt.xlim(xx.min(), xx.max())\n",
    "plt.ylim(yy.min(), yy.max())\n",
    "plt.xticks(())\n",
    "plt.yticks(())\n",
    "plt.title('Test set')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
